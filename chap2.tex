\chapter{Apple's Implementation}

A lot of Apple's source code is \href{''http://www.opensource.apple.com''}{open-sourced} which includes their implementation of the Objective-C run-time among others.

First thing that comes across mind when studying Apple's source codes is its history - there's a lot of historical code here that ensures backward compatibility. You can find, for example, some code from the NeXT era and a port to Windows - NeXT had a set of APIs called OpenStep (which is the predecessor of today's Cocoa framework on OS X), which was written in Objective-C and was aiming to run on virtually any reasonable system\footnote{http://en.wikipedia.org/wiki/OpenStep}. Also, some of Apple's own software for Windows, such as Safari, was written in Objective-C, hence the run-time needed to be compilable under Windows as well\footnote{http://jongampark.wordpress.com/2009/02/24/safari-4-beta-for-windows/}.

While the legacy code is understandable as it is required to maintain binary compatibility of all existing binaries and the portability is generally an objective of this work, neither is done in a very clean fashion.

All code is duplicated, one part serving compilation of the old run-time and the second part serving compilation of the new run-time. For example, you can find files such as \verb=objc-runtime-old.m= and \verb=objc-runtime-new.mm= (note the \verb=.mm= file extension on the new run-time file suggesting presence of C++, which will be described later).

\section{Portability}

The portability of the run-time can be divided in two parts - portability in terms of operating systems and in terms of the CPU architecture.

\subsection{CPU Portability}

Historically, Objective-C has been run on quite a few architectures - Intel x86, and Sun microchips from the NeXT era, PPC, Intel x86-64 and more recently ARM chips.

While Apple uses a lot of \verb=#if-#else= macro statements detecting whether the machine is running a 64-bit Intel processor, this is often merely to detect whether the old run-time 1.x is available (the old run-time 1.x is available only on 32-bit Intel architectures).

Where it gets into CPU portability is the \verb=objc_msgSend= function and its relatives. To ensure maximum speeds, the \verb=objc_msgSend= function is written in assembler and hence has to be rewritten for each new architecture.

The OS portability is ensured by rather a large \verb=\#ifdef= - \verb=\#else= - \verb=#endif= statements that use macros and static inline functions to define aliases to some OS X-specific functions on Windows, such as \verb=malloc_zone_malloc= or even POSIX-specific functions, like \verb=issetguid=. I will elaborate on these two examples.

\subsection{Example 1 - malloc}
On OS X, the malloc function has an extension to support memory zones\footnote{http://developer.apple.com/library/Mac/\#documentation/Darwin/Reference/

ManPages/man3/malloc\_zone\_malloc.3.html} - this way you can create multiple zones (which are really just heaps), which can get destroyed entirely at once. It has a very limited usage nowadays, but back in the day when computers had only a little memory, it was useful to allocate temporary objects (e.g.\ during a specific calculation) in its own zone, freeing it as a whole when you were done with these objects. For example, the NSMenu, which is a class representing a menu on OS X, has a class method +menuZone which returns a zone that is used for menu allocations - considering that a menu is an UI element that gets displayed on the screen for only a short period of time, it is a good idea to store all memory used to represent it in a separate zone, freeing all the memory when the menu is dismissed and hence preventing memory fragmentation. Unfortunately, when releasing a chunk of memory like this at once, the \verb=-dealloc= method doesn't get called, which could result in memory leaks. The zones were also supposed to be used to add garbage collection support to Objective-C back in the NeXT days, which never happened, though.

As Windows supports only the regular \verb=malloc= function, this had to be solved - and it has been solved rather radically, by defining the \verb=malloc_*= methods as static inline methods that call the Windows API functions\footnote{objc-os.h}:

\begin{verbatim}
static inline void *malloc_zone_malloc(malloc_zone_t z, 
                                              size_t size) { 
    return malloc(size); 
}
\end{verbatim}

While it's functional, if Microsoft ever decided to implement zones into its version of malloc, it would break the code as it would result in duplicate symbol definition. Moreover, there is another function defined in the source code\footnote{objc-class.m}:

\begin{verbatim}
void *_malloc_internal(size_t size) {
    return malloc_zone_malloc(_objc_internal_zone(), size);
}
\end{verbatim}

It would, of course, make much more sense to simply declare these internal functions that would deal with the portability issue themselves (i.e.\ moving the \verb=#if=-\verb=#else= construct here, inside each function would result in a more readable code).

\subsection{Example 2 - issetguid}
The other example is how is the \verb=issetguid= function transferred to the Windows environment:

\begin{verbatim}
#define issetugid() 0
\end{verbatim}

\section{Limitations}
There are several limitations that can be found in the code:

\subsection{16B Object Minimum Size}
Apple supplies several large libraries (or as they call them, frameworks), that a vast majority of applications build upon. In particular, the CoreFoundation and Foundation frameworks are used basically by every application in the system (if not directly, then at least indirectly).

CoreFoundation, although it implements many Objective-C classes as can be easily verified using the class-dump tool\footnote{http://www.codethecode.com/projects/class-dump/}, only provides C exports and headers, with the Objective-C classes being used privately.

On the other hand, the Foundation framework, CoreFoundation's counterpart, is mainly an Objective-C framework, providing object-oriented access to the mainly C-based CoreFoundation and officially allows access to some Objective-C classes declared in CoreFoundation.

For example, \verb=CFStringRef= is a pointer to a structure used by the CoreFoundation framework to represent a string. The Foundation framework has a \verb=NSString= class, which does generally the same. In order to prevent unnecessary code duplication as well as data conversions when using CoreFoundation functions (which accept \verb=CFStringRef=) from Objective-C code, toll-free bridging has been introduced.

There is an intricate mechanism behind it, which requires direct support from the Foundation and CoreFoundation frameworks (so you cannot create toll-free bridge of other yet-non-bridged (Core)Foundation classes)\footnote{You can read more about it here: http://www.mikeash.com/pyblog/friday-qa-2010-01-22-toll-free-bridging-internals.html}, but instances of classes that do support toll-free bridging, can be simply casted to their CoreFoundation counterpart and vice versa. Using the \verb=CFStringRef=/\verb=NSString= couple, this code if fully valid:

\begin{verbatim}
NSString *myString = @"Hello World!";
CFStringRef duplicatedString = CFStringCreateCopy(NULL, 
                                    (CFStringRef)myString);
NSString *duplicatedString2 = (NSString*)duplicatedString;
\end{verbatim}

While this is very convenient to every OS X (or iOS) developer, it poses an unexpected limitation: all object instances need to have the same minimum size as CoreFoundation ``objects" - 16 bytes, which is noted in a comment in \verb=objc-class.m= file within the \verb=_class_createInstancesFromZone= function. True that 16 bytes isn't that much in these days, but given that you could fit 4 regular objects into one on a 32-bit computer, the space adds up.

\subsection{Dynamic Loader Support}
As it's faster to simply copy the data from the binary than to construct the classes using the run-time functions (on OS X from the \verb=__OBJC= section in particular), Apple's implementation contains a set of functions that are called by the dynamic loader (\verb=dyld=) to load classes from the binary image, link them to their superclasses and to register them with the run-time.

This, however, adds \verb=dyld= to the list of library dependencies, which is transitively required by \verb=libSystem=\footnote{Basic system library on OS X that every application needs to be linked to.} anyway, but it adds dependencies to the code itself.

More portable code would declare structures to be passed to the run-time when loading a binary, and the dynamic loader would pass those to the run-time. Instead, the dynamic loader passes the binary image header pointer to the run-time, making the run-time crawl through the binary image headers for Objective-C data.

In general, the dynamic loader support in Apple's run-time poses a question, which party is responsible exactly for what part of the binary loading. It also puts a light on the binary compatibility question. If the internal structures representing a class change, the binary will not launch.

Creating classes manually using the run-time methods ensures binary compatibility, while slows down binary loading significantly and poses a question where should be the class information stored. One option would be for the compiler to generate a function with a specific name that's called by the dynamic loader, or calling it manually before anything else in the \verb=main= of your program (which might interfere with the \verb=__attribute__((constructor))= functions, though).

\subsection{C++ Influences}
The newest parts of the Objective-C run-time use many C++ features, such as methods on structures, some C++ classes, e.g. \verb=vector= and the run-time tries to unify Objective-C and C++ exceptions using the unwind library.

While this may help to clean up the code a little, it adds additional dependencies on C++ libraries.

\section{Summary}
Apple's implementation is riddled with a lot of obscure code and other very specific details - for example, when the class images are stored in the \verb=__OBJC= binary section, the superclass field is pointing to a string containing the name of that superclass - when the run-time is connecting the images, it's casting the superclasses pointer to char*, to read the superclass name, which is not a very clean solution (for example, the \'Etoil'e run-time uses two differently named structures for a cleaner cast). Moreover, the documentation is very brief, not every function has a description of what it exactly does, or only has a very short note that it is used from a different function somewhere else in the code. As will be described below, the GCC version of the run-time is much better documented in this matter.
